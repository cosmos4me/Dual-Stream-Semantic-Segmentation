# DualStream-DFormer: Multi-Modal Semantic Segmentation

[![PyTorch](https://img.shields.io/badge/PyTorch-EE4C2C?style=flat-square&logo=pytorch&logoColor=white)](https://pytorch.org/)
[![Transformers](https://img.shields.io/badge/HuggingFace-Transformers-orange?style=flat-square&logo=huggingface)](https://huggingface.co/)
[![License](https://img.shields.io/badge/License-MIT-blue.svg)](https://opensource.org/licenses/MIT)

**DualStream-DFormer**ëŠ” ê³ í•´ìƒë„ ìœ„ì„± ì´ë¯¸ì§€(Vision)ì™€ ëŒ€ê¸° ì˜¤ì—¼ ë°ì´í„°(Air Quality)ë¥¼ ìœµí•©í•˜ì—¬ ì •ë°€í•œ ì˜ì—­ ë¶„í• (Semantic Segmentation)ì„ ìˆ˜í–‰í•˜ëŠ” í•˜ì´ë¸Œë¦¬ë“œ ë”¥ëŸ¬ë‹ ëª¨ë¸ìž…ë‹ˆë‹¤. 

ì„œë¡œ ë‹¤ë¥¸ í•´ìƒë„ì™€ íŠ¹ì„±ì„ ê°€ì§„ ì´ì¢… ë°ì´í„°ë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ê²°í•©í•˜ê¸° ìœ„í•´ **SegFormer**ì™€ **ConvNeXt**ë¥¼ ê¸°ë°˜ìœ¼ë¡œ í•œ ë“€ì–¼ ìŠ¤íŠ¸ë¦¼ êµ¬ì¡°ë¥¼ ì±„íƒí•˜ì˜€ìœ¼ë©°, **FiLM**, **Gating**, **Cross-Attention** ë©”ì»¤ë‹ˆì¦˜ì„ í†µí•´ íŠ¹ì§•ì„ ë‹¨ê³„ë³„ë¡œ ìœµí•©í•©ë‹ˆë‹¤.

## ðŸŒŸ Key Features

* **Dual-Stream Architecture**:
    * **Vision Stream**: `SegFormer-B1` (Pretrained)ì„ ì‚¬ìš©í•˜ì—¬ ì‹œê°ì  íŠ¹ì§• ì¶”ì¶œ
    * **Air Quality Stream**: Custom `ConvNeXt` Encoderë¥¼ ì‚¬ìš©í•˜ì—¬ 48ì±„ë„ ëŒ€ê¸° ë°ì´í„° ì²˜ë¦¬
* **Advanced Feature Fusion**:
    * **Low-Level**: `FiLM (Feature-wise Linear Modulation)` + `Dynamic Fusion Gate`
    * **High-Level**: `Cross-Attention Block` (Global Context Modeling)
* **Robust Decoder Design**:
    * **ASPP (Atrous Spatial Pyramid Pooling)**: ë©€í‹° ìŠ¤ì¼€ì¼ ë¬¸ë§¥ í¬ì°©
    * **Attention Gates**: Skip Connection ì •ë³´ì˜ ì„ íƒì  ìœµí•©
    * **ConvNeXt Refinement**: ë””ì½”ë”© ë‹¨ê³„ì—ì„œì˜ ì„¸ë°€í•œ íŠ¹ì§• ë³µì›
* **Loss Function Strategy**:
    * **Hybrid Loss**: Focal Loss + Lovasz-Softmax Loss (Class Imbalance í•´ê²°)
    * **Deep Supervision**: ì¤‘ê°„ ë ˆì´ì–´(Auxiliary Heads)ì—ì„œë„ ì†ì‹¤ì„ ê³„ì‚°í•˜ì—¬ í•™ìŠµ ì•ˆì •ì„± í™•ë³´

## ðŸ—ï¸ Model Architecture

| Stage | Interaction Method | Purpose |
| :--- | :--- | :--- |
| **Encoder** | SegFormer (Vis) + ConvNeXt (Air) | ê°ê°ì˜ ëª¨ë‹¬ë¦¬í‹°ì—ì„œ ê³„ì¸µì  íŠ¹ì§• ì¶”ì¶œ |
| **Stage 1-2** | **FiLM + Gating** | ì±„ë„ë³„ íŠ¹ì§• ë³€ì¡° ë° ì§€ì—­ì  ì •ë³´ ìœµí•© |
| **Stage 3-4** | **Cross-Attention** | ì „ì—­ì  ë¬¸ë§¥ ì •ë³´ êµí™˜ ë° ìƒí˜¸ ì—°ê´€ì„± í•™ìŠµ |
| **Decoder** | **ASPP + Attention Gate** | ê²½ê³„ë©´ ì •ì œ ë° í•´ìƒë„ ë³µì› |

## ðŸ“‚ Directory Structure

ë°ì´í„°ì…‹ì€ `DualStreamDataset` í´ëž˜ìŠ¤ì— ë§žì¶° ë‹¤ìŒê³¼ ê°™ì€ êµ¬ì¡°ë¡œ êµ¬ì„±ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.

```plaintext
/content/
â”œâ”€â”€ ts_sn/      # Train Images (Satellite)
â”œâ”€â”€ tl_sn/      # Train Labels (Masks)
â”œâ”€â”€ t_ap/       # Train Air Pollution Data (TIF)
â”œâ”€â”€ t_gems/     # Train GEMS Data (TIF)
â”œâ”€â”€ vs_sn/      # Validation Images
â”œâ”€â”€ vl_sn/      # Validation Labels
â”œâ”€â”€ v_ap/       # Validation Air Pollution Data
â””â”€â”€ v_gems/     # Validation GEMS Data

âš™ï¸ Configuration
LEARNING_RATE = 3.0e-4
BATCH_SIZE = 4
EPOCHS = 40
TARGET_SIZE = 512
NUM_CLASSES = 2 (Background / Target)

# Loss Weights
FOCAL_WEIGHT = 0.5
LOVASZ_WEIGHT = 0.5
AUX_LOSS_WEIGHTS = {'final': 1.0, 'f2': 0.3, 'f3': 0.15}

ðŸš€ Usage
1. Requirements
pip install torch torchvision rasterio opencv-python transformers tqdm

2. Training
from torch.utils.data import DataLoader
import torch.optim as optim

# ë””ë°”ì´ìŠ¤ ì„¤ì •
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# ë°ì´í„°ì…‹ ë° ë¡œë”
train_dataset = DualStreamDataset(TR_IMG_DIR, TR_LAB_DIR, TR_AP_DIR, TR_GEMS_DIR)
train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)

# ëª¨ë¸ ì´ˆê¸°í™”
model = DualStream_DFormer_Model(num_classes=1).to(device)

# ì˜µí‹°ë§ˆì´ì € ë° ìŠ¤ì¼€ì¼ëŸ¬
optimizer = optim.AdamW(model.parameters(), lr=LEARNING_RATE)
scaler = torch.cuda.amp.GradScaler()

# í•™ìŠµ ë£¨í”„
model.train()
for epoch in range(EPOCHS):
    loss = train_one_epoch(model, train_loader, optimizer, scaler, device)
    print(f"Epoch {epoch+1} Loss: {loss:.4f}")
    
    # ì²´í¬í¬ì¸íŠ¸ ì €ìž¥
    if (epoch + 1) % 5 == 0:
        torch.save(model.state_dict(), f"{CHECKPOINT}_ep{epoch+1}.pth")
3. Inference
model.eval()
with torch.no_grad():
    output = model(pixel_values=test_img, air_values=test_air)
    logits = output['logits']
    prediction = (torch.sigmoid(logits) > 0.5).long()
